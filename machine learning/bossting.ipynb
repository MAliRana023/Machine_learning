{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVXykwbtXQOQ",
        "outputId": "8fdbcde1-d651-48e5-9f8f-4e3ae6590a63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.11.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "!pip install xgboost\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Load the diamond dataset\n",
        "diamonds_df = sns.load_dataset('diamonds')\n",
        "\n",
        "# Preprocess the data if necessary (e.g., encoding categorical variables)\n",
        "# Encode categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "diamonds_df['cut'] = label_encoder.fit_transform(diamonds_df['cut'])\n",
        "diamonds_df['color'] = label_encoder.fit_transform(diamonds_df['color'])\n",
        "diamonds_df['clarity'] = label_encoder.fit_transform(diamonds_df['clarity'])\n",
        "\n",
        "# Split data into features (X) and target (y)\n",
        "X = diamonds_df.drop('price', axis=1)\n",
        "y = diamonds_df['price']\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train AdaBoost Regressor\n",
        "adaBoost_regressor = AdaBoostRegressor()\n",
        "adaBoost_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Train XGBoost Regressor\n",
        "xgBoost_regressor = XGBRegressor()\n",
        "xgBoost_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "randomForest_regressor = RandomForestRegressor()\n",
        "randomForest_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the performance of each regressor\n",
        "adaBoost_predictions = adaBoost_regressor.predict(X_test)\n",
        "xgBoost_predictions = xgBoost_regressor.predict(X_test)\n",
        "randomForest_predictions = randomForest_regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error for each regressor\n",
        "adaBoost_mse = mean_squared_error(y_test, adaBoost_predictions)\n",
        "xgBoost_mse = mean_squared_error(y_test, xgBoost_predictions)\n",
        "randomForest_mse = mean_squared_error(y_test, randomForest_predictions)\n",
        "\n",
        "print(\"Mean Squared Error (AdaBoost):\", adaBoost_mse)\n",
        "print(\"Mean Squared Error (XGBoost):\", xgBoost_mse)\n",
        "print(\"Mean Squared Error (Random Forest):\", randomForest_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rarv57RfXTAz",
        "outputId": "db18f519-6409-4741-c230-7c252cce2642"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (AdaBoost): 1670634.8562896068\n",
            "Mean Squared Error (XGBoost): 297651.2769023406\n",
            "Mean Squared Error (Random Forest): 294801.2253448156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Accessing XGBoost regressor parameters\n",
        "xgBoost_params = xgBoost_regressor.get_params()\n",
        "\n",
        "# Print XGBoost regressor parameters\n",
        "print(\"XGBoost Regressor Parameters:\", xgBoost_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VycQWIovXW4Z",
        "outputId": "6e479874-701c-4a46-cd34-8a2a3e731282"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Regressor Parameters: {'objective': 'reg:squarederror', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': None, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': None, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': None, 'validate_parameters': None, 'verbosity': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "diamonds_df = sns.load_dataset('diamonds')\n",
        "\n",
        "# Preprocess the data if necessary (e.g., encoding categorical variables)\n",
        "# Encode categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "diamonds_df['cut'] = label_encoder.fit_transform(diamonds_df['cut'])\n",
        "diamonds_df['color'] = label_encoder.fit_transform(diamonds_df['color'])\n",
        "diamonds_df['clarity'] = label_encoder.fit_transform(diamonds_df['clarity'])\n",
        "\n",
        "# Split data into features (X) and target (y)\n",
        "X = diamonds_df.drop('price', axis=1)\n",
        "y = diamonds_df['price']\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grid for Random Forest Regressor\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Initialize Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor()\n",
        "\n",
        "# Perform Grid Search for Random Forest\n",
        "rf_grid_search = GridSearchCV(rf_regressor, rf_param_grid, cv=5)\n",
        "rf_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best score for Random Forest\n",
        "best_rf_params = rf_grid_search.best_params_\n",
        "best_rf_score = rf_grid_search.best_score_\n",
        "\n",
        "print(\"Best parameters for Random Forest:\", best_rf_params)\n",
        "print(\"Best score for Random Forest:\", best_rf_score)\n",
        "\n",
        "# Define parameter grid for XGBoost Regressor\n",
        "xgb_param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# Initialize XGBoost Regressor\n",
        "xgb_regressor = XGBRegressor()\n",
        "\n",
        "# Perform Grid Search for XGBoost\n",
        "xgb_grid_search = GridSearchCV(xgb_regressor, xgb_param_grid, cv=5)\n",
        "xgb_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best score for XGBoost\n",
        "best_xgb_params = xgb_grid_search.best_params_\n",
        "best_xgb_score = xgb_grid_search.best_score_\n",
        "\n",
        "print(\"Best parameters for XGBoost:\", best_xgb_params)\n",
        "print(\"Best score for XGBoost:\", best_xgb_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zou_mAbYXLT",
        "outputId": "582d2c1d-7399-4879-fc4e-7ac25b3a640f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for Random Forest: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 150}\n",
            "Best score for Random Forest: 0.9806695474421693\n",
            "Best parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 150}\n",
            "Best score for XGBoost: 0.9815949553680431\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import  RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Load the diamond dataset\n",
        "diamonds_df = sns.load_dataset('diamonds')\n",
        "\n",
        "# Preprocess the data if necessary (e.g., encoding categorical variables)\n",
        "# Encode categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "diamonds_df['cut'] = label_encoder.fit_transform(diamonds_df['cut'])\n",
        "diamonds_df['color'] = label_encoder.fit_transform(diamonds_df['color'])\n",
        "diamonds_df['clarity'] = label_encoder.fit_transform(diamonds_df['clarity'])\n",
        "\n",
        "# Split data into features (X) and target (y)\n",
        "X = diamonds_df.drop('price', axis=1)\n",
        "y = diamonds_df['price']\n",
        "#define the model\n",
        "rf_regressor = RandomForestRegressor()\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train AdaBoost Regressor\n",
        "adaBoost_regressor = AdaBoostRegressor()\n",
        "adaBoost_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Train XGBoost Regressor\n",
        "xgBoost_regressor = XGBRegressor()\n",
        "xgBoost_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "randomForest_regressor = RandomForestRegressor()\n",
        "randomForest_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the performance of each regressor\n",
        "adaBoost_predictions = adaBoost_regressor.predict(X_test)\n",
        "xgBoost_predictions = xgBoost_regressor.predict(X_test)\n",
        "randomForest_predictions = randomForest_regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error for each regressor\n",
        "adaBoost_mse = mean_squared_error(y_test, adaBoost_predictions)\n",
        "xgBoost_mse = mean_squared_error(y_test, xgBoost_predictions)\n",
        "randomForest_mse = mean_squared_error(y_test, randomForest_predictions)\n",
        "\n",
        "print(\"Mean Squared Error (AdaBoost):\", adaBoost_mse)\n",
        "print(\"Mean Squared Error (XGBoost):\", xgBoost_mse)\n",
        "print(\"Mean Squared Error (Random Forest):\", randomForest_mse)\n"
      ],
      "metadata": {
        "id": "lh2nKNtbfQeo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}